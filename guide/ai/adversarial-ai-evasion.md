---
icon: ufo
---

# Adversarial AI Evasion

## ‚öîÔ∏è **Adversarial AI Evasion ‚Äî Image & Text Perturbation Challenges for CTFs**

> _‚ÄúIf you can‚Äôt break the model, teach the input to lie.‚Äù_
>
> This guide dives deep into **adversarial evasion**, one of the most fun and puzzling areas in AI-themed CTFs.\
> You‚Äôll learn how competitors craft, detect, and defend against subtle input perturbations that fool AI models ‚Äî ethically and safely in lab settings.

***

### I. üß© **Understanding Evasion Challenges**

| Category                | Objective                                              | Example Task               |
| ----------------------- | ------------------------------------------------------ | -------------------------- |
| **Image Evasion**       | Create minimal pixel changes that flip classification  | Turn ‚Äúcat‚Äù ‚Üí ‚Äúdog‚Äù         |
| **Text Evasion**        | Alter words or encodings to fool sentiment/spam filter | ‚ÄúY0u are amaz!ng‚Äù          |
| **Audio Evasion**       | Embed trigger sounds to cause misrecognition           | Hidden phrase in speech    |
| **Vector Perturbation** | Modify embeddings to evade anomaly detector            | Alter feature magnitudes   |
| **Model Bypass**        | Input pattern triggers undesired branch                | Adversarial patch or token |

CTFs simulate these as **puzzles or pattern-reversal tasks** ‚Äî find an input that breaks the model or detect adversarial ones.

***

### II. ‚öôÔ∏è **Toolbox for Adversarial ML**

| Purpose             | Tools                                             |
| ------------------- | ------------------------------------------------- |
| Image attacks       | `Foolbox`, `Adversarial Robustness Toolbox (ART)` |
| Text attacks        | `TextAttack`, `OpenAttack`, `CheckList`           |
| Model visualization | `TensorBoard`, `Netron`                           |
| Feature inspection  | `numpy`, `matplotlib`, `pandas`                   |
| Defense evaluation  | `adversarial-robustness-toolbox`, `CleverHans`    |
| Forensics           | `scikit-learn`, `shap`, `lime`                    |

***

### III. üß† **Image Evasion: The Classics**

#### 1Ô∏è‚É£ **Fast Gradient Sign Method (FGSM)**

Compute minimal perturbation in gradient direction:

```python
x_adv = x + epsilon * sign(‚àáx L(model(x), y))
```

**CTF Objective:** find the smallest `epsilon` where the model misclassifies the image.

üß† _Flag often equals pixel index or epsilon value where misclassification first occurs._

***

#### 2Ô∏è‚É£ **Projected Gradient Descent (PGD)**

Iteratively applies FGSM within epsilon-ball.

#### 3Ô∏è‚É£ **Adversarial Patch**

A visible localized region triggers specific output regardless of the rest of image.

| Task         | Hint                                           |
| ------------ | ---------------------------------------------- |
| Detect Patch | Unusual brightness or block pattern            |
| Create Patch | Alter specific coordinates (CTF-provided mask) |

***

#### 4Ô∏è‚É£ **One-Pixel Attack**

Change a single pixel to flip classification.

CTF version: you‚Äôre given model weights, asked to brute-force pixel location that flips label ‚Üí flag is `(x,y)` coordinates.

***

### IV. üß© **Textual Adversarial Evasion**

| Technique                 | Example                        | Notes                             |
| ------------------------- | ------------------------------ | --------------------------------- |
| **Character-Level**       | ‚Äúlove‚Äù ‚Üí ‚Äúl0ve‚Äù, ‚Äúl‚ô°ve‚Äù        | Unicode homograph                 |
| **Word-Level Synonyms**   | ‚Äúhappy‚Äù ‚Üí ‚Äúglad‚Äù               | Context shift                     |
| **Sentence Paraphrasing** | Reordering or rephrasing       | Changes syntax, preserves meaning |
| **Encoding Tricks**       | Base64, URL, zero-width spaces | Bypass filters                    |

CTFs test:

* Can you bypass a filter that blocks ‚Äúflag‚Äù?
* Can you detect which text samples were poisoned?
* Can you restore obfuscated sentences?

üß† **Tools:** `TextAttack`, `OpenAttack`, `spaCy`, `transformers`.

***

#### Example CTF Script

```python
from textattack.augmentation import WordNetAugmenter
aug = WordNetAugmenter()
print(aug.augment("This message contains the flag"))
```

Outputs variant sentences that might fool a rule-based classifier.

***

### V. üß† **Audio & Signal Evasion**

| Attack                      | Concept                           |
| --------------------------- | --------------------------------- |
| **Hidden Command**          | Embed speech at sub-audible level |
| **Spectrogram Patch**       | Frequency-space manipulation      |
| **Time-based Perturbation** | Reordering sample frames          |

**CTF tasks:** detect altered audio or reconstruct hidden waveform.\
Use `spek`, `sox`, `librosa` for analysis.

```python
import librosa, matplotlib.pyplot as plt
y,sr = librosa.load('challenge.wav')
plt.specgram(y,Fs=sr)
```

***

### VI. üî¨ **Adversarial Feature-Space Manipulation**

| Task                       | Technique                             |
| -------------------------- | ------------------------------------- |
| Modify embeddings to evade | Add noise along non-critical PCA axes |
| Fool anomaly detector      | Scale features to boundary            |
| Recreate hidden pattern    | Reverse-engineer trigger vector       |

**CTF pattern:** given vector arrays; you must alter them until model outputs `target_label`.

***

### VII. ‚öîÔ∏è **Detecting Adversarial Inputs (Defensive CTFs)**

| Detection Strategy            | Concept                                                       |
| ----------------------------- | ------------------------------------------------------------- |
| **Statistical Outlier Tests** | Adversarial samples deviate in pixel / embedding distribution |
| **Confidence Analysis**       | Classifier overconfident on nonsensical input                 |
| **Gradient Norms**            | High sensitivity indicates perturbation                       |
| **Input Reconstruction**      | Denoising autoencoder highlights changes                      |
| **Frequency Analysis**        | Adversarial noise shows unusual high-frequency components     |

#### Example

```python
import numpy as np
diff = np.mean(abs(clean - suspect))
if diff > 0.05:
    print("Adversarial candidate")
```

***

### VIII. üß© **CTF Design Patterns**

| Challenge                  | Description                                            |
| -------------------------- | ------------------------------------------------------ |
| **‚ÄúInvisible Noise‚Äù**      | Recover clean image hidden behind perturbation         |
| **‚ÄúClassifier Blindspot‚Äù** | Input that bypasses model logic                        |
| **‚ÄúPatchwork Flag‚Äù**       | Combine fragments of adversarial patches to form flag  |
| **‚ÄúPerturbation Budget‚Äù**  | Minimal L2 difference that breaks model ‚Üí value = flag |
| **‚ÄúDetector vs Attacker‚Äù** | Submit adversarial sample that evades detection net    |

***

### IX. üß† **Evaluation Metrics**

| Metric                       | Meaning                          |
| ---------------------------- | -------------------------------- |
| L‚àû / L2 Norm                 | Perturbation magnitude           |
| Confidence Drop              | Change in prediction probability |
| Attack Success Rate (ASR)    | % of successful fooling inputs   |
| Structural Similarity (SSIM) | Image perceptual change          |
| BLEU / Perplexity            | Text meaning preservation        |

***

### X. üß∞ **Practical Libraries**

| Library                          | Language | Function                     |
| -------------------------------- | -------- | ---------------------------- |
| `foolbox`                        | Python   | FGSM, PGD, CW, DeepFool      |
| `Adversarial-Robustness-Toolbox` | Python   | 40+ attack & defense methods |
| `TextAttack`                     | Python   | NLP adversarial framework    |
| `OpenAttack`                     | Python   | Benchmark text attacks       |
| `Torchattacks`                   | Python   | Simple PyTorch-based attacks |
| `cleverhans`                     | Python   | Classic research toolkit     |

***

### XI. ‚öôÔ∏è **Example: FGSM in a CTF**

Given model and image `input.png`:

```python
from foolbox import PyTorchModel, accuracy, samples
import torch
epsilon = 0.01
x = torch.tensor(img)
x_adv = x + epsilon * x.grad.sign()
```

Submit resulting `x_adv` that flips model‚Äôs output ‚Äî verify misclassification.

Flag could be `"epsilon=0.01"` or image checksum.

***

### XII. üß© **Visual Forensics (Detecting Evasion)**

* Compute pixel difference map ‚Üí `abs(orig - adv)`
* FFT or DCT ‚Üí adversarial noise often shows uniform frequency spread.
* Statistical moment shift ‚Üí variance up, skew changes.

```python
import numpy as np
np.mean(abs(orig-adv)), np.var(orig-adv)
```

***

### XIII. üß† **Advanced Scenarios**

| Challenge                  | Concept                                         |
| -------------------------- | ----------------------------------------------- |
| **Multi-Modal Evasion**    | Fool both text & image classifier               |
| **Adversarial CAPTCHA**    | Generate inputs bypassing vision + NLP filters  |
| **Zero-Knowledge Evasion** | No model access, use query feedback             |
| **Physical Attacks**       | Printed patch misclassifies camera input        |
| **Multi-Step CTF Chain**   | Combine data poisoning ‚Üí evasion ‚Üí exfiltration |

***

### XIV. üß± **CTF Workflow Summary**

```
1Ô∏è‚É£ Identify model type & task (image/text/audio)
2Ô∏è‚É£ Load clean sample & test prediction
3Ô∏è‚É£ Apply gradient or transformation
4Ô∏è‚É£ Measure minimal perturbation for flip
5Ô∏è‚É£ Verify perceptual similarity
6Ô∏è‚É£ Extract flag (perturbation, pattern, coordinate)
```

***

### XV. ‚ö° **Pro Tips**

* Visualize everything ‚Äî adversarial changes are easier to see than to guess.
* Normalize inputs before comparing pixel differences.
* Test both black-box (API only) and white-box (weights provided).
* Keep epsilon small ‚Äî many CTFs require minimal visible noise.
* Record random seeds; reproducibility is part of flag verification.
* For text, prefer semantically consistent substitutions.

***

### XVI. üìö **Further Reading**

* Goodfellow et al., _Explaining and Harnessing Adversarial Examples_
* MITRE ATLAS ‚Üí [ML Evasion Techniques](https://atlas.mitre.org/)
* Adversarial Robustness Toolbox Docs
* TextAttack Research Paper (Morris et al. 2020)
* OpenAI Red Team Reports on LLM Prompt Evasion
* DEF CON AI Village ‚ÄúAdversarial Image Labs‚Äù

***
